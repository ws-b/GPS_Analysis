import logging
from pathlib import Path
from tqdm import tqdm
import pandas as pd
import pickle
import xgboost as xgb
import sys
import os
import multiprocessing
from dotenv import load_dotenv
from functools import partial  # functools.partial ì„í¬íŠ¸

# --- .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ---
load_dotenv()

# --- íŒŒì´ì¬ ê²½ë¡œ ì„¤ì • ë° ë‹¤ë¥¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from Source import config
from Source import data_handler

# --- ë¡œê¹… ì„¤ì • ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(processName)s - %(levelname)s - %(message)s')

# ==============================================================================
#  ì²˜ë¦¬ ì™„ë£Œ ì—¬ë¶€ ê²€ì‚¬ í•¨ìˆ˜
# ==============================================================================
def check_file_completion(file_path, step):
    """
    ì§€ì •ëœ ë‹¨ê³„(step)ì— ë”°ë¼ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    (ì´ í•¨ìˆ˜ëŠ” ë³‘ë ¬ ì²˜ë¦¬ì˜ ì‘ì—… ë‹¨ìœ„ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤)
    """
    try:
        if step == 2:
            cols_to_read = ['ext_temp']
        elif step == 3:
            cols_to_read = ['Power_hybrid']
        else:
            return False # ì•Œ ìˆ˜ ì—†ëŠ” ë‹¨ê³„ëŠ” ë¯¸ì™„ë£Œë¡œ ì²˜ë¦¬

        df = pd.read_csv(file_path, usecols=cols_to_read, low_memory=False)

        if step == 2:
            if 'ext_temp' in df.columns and not df.empty:
                if not pd.isna(df['ext_temp'].iloc[0]) and not pd.isna(df['ext_temp'].iloc[-1]):
                    return True
            return False
        elif step == 3:
            return 'Power_hybrid' in df.columns
    except (FileNotFoundError, pd.errors.EmptyDataError, ValueError): # ì—´ì´ ì—†ì„ ë•Œ ValueError ë°œìƒ ê°€ëŠ¥
        return False
    except Exception as e:
        logging.debug(f"'{file_path.name}' íŒŒì¼ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

# ==============================================================================
#  ì²˜ë¦¬ ëŒ€ìƒ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
# ==============================================================================
def get_files_to_process(step):
    """
    ì§€ì •ëœ ë‹¨ê³„ì— ëŒ€í•´ ì²˜ë¦¬í•´ì•¼ í•  íŒŒì¼ ëª©ë¡ì„ ë³‘ë ¬ë¡œ ì‹ ì†í•˜ê²Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    all_files = list(config.PATHS['processed_koti'].glob("*.csv"))
    if not all_files:
        logging.info(f"'{config.PATHS['processed_koti']}' ê²½ë¡œì— ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return []

    # partialì„ ì‚¬ìš©í•˜ì—¬ check_file_completion í•¨ìˆ˜ì— 'step' ì¸ìë¥¼ ê³ ì •
    worker_func = partial(check_file_completion, step=step)
    
    files_to_process = []
    # CPU ì½”ì–´ ìˆ˜ë§Œí¼ í”„ë¡œì„¸ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ ë³‘ë ¬ë¡œ ì‘ì—… ì‹¤í–‰
    num_processes = max(1, os.cpu_count() - 1)
    with multiprocessing.Pool(processes=num_processes) as pool:
        # pool.imap_unorderedë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—… ì™„ë£Œ ìˆœì„œëŒ€ë¡œ ê²°ê³¼ë¥¼ ë°›ê³  tqdmìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ
        results = list(tqdm(pool.imap_unordered(worker_func, all_files), 
                              total=len(all_files), 
                              desc=f"ë³‘ë ¬ ê²€ì‚¬ (ë‹¨ê³„ {step})"))
    
    # ê²°ê³¼(True/False ë¦¬ìŠ¤íŠ¸)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ í•  íŒŒì¼ ëª©ë¡ì„ í•„í„°ë§
    files_to_process = [file for file, is_complete in zip(all_files, results) if not is_complete]

    logging.info(f"ì´ {len(all_files)}ê°œ íŒŒì¼ ì¤‘ {len(files_to_process)}ê°œê°€ {step}ë‹¨ê³„ ì²˜ë¦¬ ëŒ€ìƒìœ¼ë¡œ ê²°ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return files_to_process

# ==============================================================================
#  ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ Worker í•¨ìˆ˜
# ==============================================================================
def worker_step2(args):
    """2ë‹¨ê³„ Worker: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë° ë¬¼ë¦¬ ëª¨ë¸ ê³„ì‚°"""
    file_path, vehicle_params, stations_df, auth_key, stop_event = args
    try:
        if stop_event.is_set():
            return "STOPPED"
        success = data_handler.add_features_and_temperature(file_path, stations_df, auth_key)
        if not success:
            logging.warning(f"API ì œí•œìœ¼ë¡œ '{file_path.name}' ì²˜ë¦¬ ì‹¤íŒ¨. ëª¨ë“  ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            stop_event.set()
            return f"API_LIMIT: {file_path.name}"
        data_handler.calculate_physics_power(file_path, vehicle_params)
        return f"SUCCESS: {file_path.name}"
    except Exception as e:
        logging.error(f"'{file_path.name}' 2ë‹¨ê³„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return f"FAILED: {file_path.name}"

def worker_step3(args):
    """3ë‹¨ê³„ Worker: í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì˜ˆì¸¡"""
    file_path, model, scaler = args
    try:
        data_handler.predict_hybrid_power(file_path, model, scaler)
        return f"SUCCESS: {file_path.name}"
    except Exception as e:
        logging.error(f"'{file_path.name}' 3ë‹¨ê³„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return f"FAILED: {file_path.name}"

# ==============================================================================
#  íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ ì‹¤í–‰ í•¨ìˆ˜
# ==============================================================================
def run_parsing_and_filtering():
    """1ë‹¨ê³„: ì›ë³¸ ë°ì´í„° íŒŒì‹±"""
    logging.info("===== 1ë‹¨ê³„: íŒŒì‹± ë° í•„í„°ë§ ì‹œì‘ =====")
    raw_dir = config.PATHS['koti_raw_data']
    output_dir = config.PATHS['processed_koti']
    raw_files = list(raw_dir.glob("*.csv"))
    if not raw_files:
        logging.warning(f"'{raw_dir}' ê²½ë¡œì— ì²˜ë¦¬í•  CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    total_saved = 0
    for raw_file in tqdm(raw_files, desc="1ë‹¨ê³„: ì›ë³¸ íŒŒì¼ ì²˜ë¦¬"):
        total_saved += data_handler.parse_and_filter_raw_data(
            raw_file, output_dir, config.VALID_YEARS, config.MIN_ROWS_FOR_VALID_FILE
        )
    logging.info(f"íŒŒì‹± ë° í•„í„°ë§ ì™„ë£Œ. ì´ {total_saved}ê°œì˜ ìœ íš¨ íŒŒì¼ ì €ì¥ë¨.")

def run_parallel_step2(vehicle_model_name):
    """2ë‹¨ê³„: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë° ë¬¼ë¦¬ ëª¨ë¸ ê³„ì‚°"""
    logging.info(f"===== 2ë‹¨ê³„: '{vehicle_model_name}' ëª¨ë¸ ê¸°ë°˜ ë¬¼ë¦¬ëŸ‰/ì „ë ¥ ê³„ì‚° =====")
    if not config.KMA_API_KEY:
        logging.error("ê¸°ìƒì²­ API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. '.env' íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    vehicle_params = config.VEHICLE_PARAMS.get(vehicle_model_name)
    if not vehicle_params:
        logging.error(f"'{vehicle_model_name}' ì°¨ëŸ‰ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    try:
        stations_df = pd.read_csv(config.PATHS['stations_csv'])
        stations_df.rename(columns={'LON':'longitude', 'LAT':'latitude'}, inplace=True)
    except FileNotFoundError:
        logging.error(f"ê¸°ìƒ ê´€ì¸¡ì†Œ íŒŒì¼ '{config.PATHS['stations_csv']}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    files_to_process = get_files_to_process(step=2)
    if not files_to_process:
        logging.info("ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ 2ë‹¨ê³„ ì²˜ë¦¬ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
        return
    manager = multiprocessing.Manager()
    stop_event = manager.Event()
    num_processes = max(1, os.cpu_count() - 1)
    tasks = [(file, vehicle_params, stations_df, config.KMA_API_KEY, stop_event) for file in files_to_process]
    with multiprocessing.Pool(processes=num_processes) as pool:
        results = list(tqdm(pool.imap_unordered(worker_step2, tasks), total=len(tasks), desc="2ë‹¨ê³„: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§"))
    success_count = sum(1 for r in results if r and r.startswith("SUCCESS"))
    fail_count = sum(1 for r in results if r and r.startswith("FAILED"))
    api_limit_count = sum(1 for r in results if r and r.startswith("API_LIMIT"))
    stopped_count = sum(1 for r in results if r and r.startswith("STOPPED"))
    logging.info(f"2ë‹¨ê³„ ì™„ë£Œ: ì„±ê³µ {success_count}, ì‹¤íŒ¨ {fail_count}, API ì œí•œ {api_limit_count}, ê°•ì œ ì¤‘ë‹¨ {stopped_count}")

def run_parallel_step3():
    """3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì˜ˆì¸¡"""
    logging.info("===== 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì˜ˆì¸¡  =====")
    try:
        model = xgb.Booster()
        model.load_model(config.PATHS['model_path'])
        with open(config.PATHS['scaler_path'], 'rb') as f:
            scaler = pickle.load(f)
    except Exception as e:
        logging.error(f"ML ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}. 3ë‹¨ê³„ë¥¼ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    files_to_process = get_files_to_process(step=3)
    if not files_to_process:
        logging.info("ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ 3ë‹¨ê³„ ì²˜ë¦¬ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
        return
    num_processes = max(1, os.cpu_count() - 1)
    tasks = [(file, model, scaler) for file in files_to_process]
    with multiprocessing.Pool(processes=num_processes) as pool:
        results = list(tqdm(pool.imap_unordered(worker_step3, tasks), total=len(tasks), desc="3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì˜ˆì¸¡"))
    success_count = sum(1 for r in results if r and r.startswith("SUCCESS"))
    fail_count = sum(1 for r in results if r and r.startswith("FAILED"))
    logging.info(f"3ë‹¨ê³„ ì™„ë£Œ: ì„±ê³µ {success_count}, ì‹¤íŒ¨ {fail_count}")

def main():
    """ë©”ì¸ ë©”ë‰´"""
    while True:
        print("\n" + "="*50)
        print("ğŸš— KOTI GPS ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ì´ˆê³ ì† ë³‘ë ¬ ê²€ì‚¬ ë²„ì „) ğŸš—")
        print("="*50)
        print("1. [ë°ì´í„° ì¤€ë¹„] íŒŒì‹± ë° í•„í„°ë§")
        print("2. [ë¬¼ë¦¬ ëª¨ë¸] í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë° ë¬¼ë¦¬ ì „ë ¥ ê³„ì‚°")
        print("3. [í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸] ML ëª¨ë¸ ê¸°ë°˜ ì „ë ¥ ì˜ˆì¸¡")
        print("--------------------------------------------------")
        print("A. ì „ì²´ íŒŒì´í”„ë¼ì¸ ìˆœì°¨ ì‹¤í–‰ (1 -> 2 -> 3)")
        print("0. í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
        choice = input("ì‹¤í–‰í•  ì‘ì—… ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").upper()
        if choice == '1':
            run_parsing_and_filtering()
        elif choice == '2':
            model_name = input(f"ë¬¼ë¦¬ ëª¨ë¸ì— ì‚¬ìš©í•  ì°¨ëŸ‰ ëª¨ë¸ëª… ì…ë ¥ ({', '.join(config.VEHICLE_PARAMS.keys())}): ")
            if model_name in config.VEHICLE_PARAMS:
                run_parallel_step2(model_name)
            else:
                logging.error("ì˜ëª»ëœ ëª¨ë¸ëª…ì…ë‹ˆë‹¤.")
        elif choice == '3':
            run_parallel_step3()
        elif choice == 'A':
            model_name = input(f"ì „ì²´ íŒŒì´í”„ë¼ì¸ì— ì‚¬ìš©í•  ì°¨ëŸ‰ ëª¨ë¸ëª… ì…ë ¥ ({', '.join(config.VEHICLE_PARAMS.keys())}): ")
            if model_name in config.VEHICLE_PARAMS:
                run_parsing_and_filtering()
                run_parallel_step2(model_name)
                run_parallel_step3()
            else:
                logging.error("ì˜ëª»ëœ ëª¨ë¸ëª…ì…ë‹ˆë‹¤.")
        elif choice == '0':
            break
        else:
            print("ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    multiprocessing.freeze_support()
    main()